{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from transformers import pipeline\n",
    "from newsplease import NewsPlease\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "emotions_class = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\n",
    "classes_class = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = NewsPlease.from_url(\"https://www.npr.org/2023/12/22/1221230635/japan-alleged-political-corruption-ldp-slush-fund\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Japan's governing Liberal Democratic Party replaced two of its top executives as part of a purge related to investigations into alleged political slush funds.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = article.title + \"\\n\" + article.maintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the text into sentences\n",
    "sentences = re.split(r'(?<=[.!?])\\s+', article_text)\n",
    "\n",
    "# Creating a dictionary with the number of characters, words, and the content of each sentence\n",
    "sentence_dict = []\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    words = sentence.split()\n",
    "    dict_append = {\n",
    "        'chars': len(sentence), \n",
    "        'words': len(words), \n",
    "        'content': sentence,\n",
    "        'emotions': {}\n",
    "    }\n",
    "    for item in emotions_class(sentence)[0]:\n",
    "        dict_append['emotions'][item['label']] = round(item['score'] * 100, 2)\n",
    "\n",
    "    sentence_dict.append(dict_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_sum_char = 0\n",
    "emotion_value = 0\n",
    "emotions_percentage = {}\n",
    "\n",
    "for emotion in ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']:\n",
    "    for sentence in sentence_dict:\n",
    "        emotion_value += sentence['chars'] * sentence['emotions'][emotion]\n",
    "        emotion_sum_char += sentence['chars']\n",
    "    emotions_percentage[emotion] = emotion_value / emotion_sum_char\n",
    "    emotion_value = 0\n",
    "    emotion_sum_char = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 12.074123116979731,\n",
       " 'disgust': 25.135253859029195,\n",
       " 'fear': 3.9958601450623035,\n",
       " 'joy': 1.4329570392412125,\n",
       " 'neutral': 49.124999070113454,\n",
       " 'sadness': 7.03720104147294,\n",
       " 'surprise': 1.2025813650734611}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.00297563697228"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_emotions = 0\n",
    "for i in emotions_percentage.values():\n",
    "    sum_emotions += i\n",
    "sum_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')  # Required for the first time\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = tokenizer.tokenize(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Party bosses fall in Japan's worst political corruption scandal in decades\n",
      "Party bosses fall in Japan's worst political corruption scandal in decades\n",
      "Enlarge this image toggle caption Kyodo via Reuters Connect Kyodo via Reuters Connect\n",
      "SEOUL, South Korea â€” Japan's governing Liberal Democratic Party (LDP) on Friday replaced two of its top executives, as part of a purge related to the worst corruption scandal to rock the country in three decades.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    if len(sentence) > 430:\n",
    "        print(sentence)\n",
    "        print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    nltk.download('punkt')  # Required for the first time\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_clusters(clusters):\n",
    "    # Define a threshold for when to redistribute (e.g., if the last cluster is less than half the average size)\n",
    "    average_size = sum(len(cluster) for cluster in clusters) / len(clusters)\n",
    "    min_size = average_size / 2\n",
    "\n",
    "    if len(clusters) > 1 and len(clusters[-1]) < min_size:\n",
    "        # Attempt to redistribute\n",
    "        last_cluster = clusters[-1].split()\n",
    "        prev_cluster = clusters[-2].split()\n",
    "\n",
    "        # While the last cluster is too short and the previous cluster has sentences to give\n",
    "        while len(' '.join(last_cluster)) < min_size and prev_cluster:\n",
    "            # Move the last sentence from the previous cluster to the beginning of the last cluster\n",
    "            last_cluster.insert(0, prev_cluster.pop())\n",
    "\n",
    "        # Update the clusters with the redistributed sentences\n",
    "        clusters[-2] = ' '.join(prev_cluster)\n",
    "        clusters[-1] = ' '.join(last_cluster)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters(sentences):\n",
    "    clusters = []\n",
    "    current_cluster = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Check if adding the next sentence would exceed the limit\n",
    "        if len(current_cluster) + len(sentence) > 512:\n",
    "            # If the current cluster is not empty, add it to clusters\n",
    "            if current_cluster:\n",
    "                clusters.append(current_cluster)\n",
    "            # Start a new cluster with the current sentence\n",
    "            current_cluster = sentence\n",
    "        else:\n",
    "            # Add a space if the cluster already has content\n",
    "            if current_cluster:\n",
    "                current_cluster += \" \"\n",
    "            current_cluster += sentence\n",
    "\n",
    "    # Don't forget to add the last cluster if it's not empty\n",
    "    if current_cluster:\n",
    "        clusters.append(current_cluster)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(article):\n",
    "    sentences = split_into_sentences(article)\n",
    "    initial_clusters = create_clusters(sentences)\n",
    "    balanced_clusters = balance_clusters(initial_clusters)\n",
    "    return balanced_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: Party bosses fall in Japan's worst political corru...\n",
      "Cluster 2: The outgoing executives were in charge of policy a...\n",
      "Cluster 3: \"Japanese democracy's strength is going to be test...\n",
      "Cluster 4: The LDP has only lost power twice in seven decades...\n",
      "Cluster 5: Prosecutors, meanwhile, are looking into allegatio...\n",
      "Cluster 6: In a statement cited by The Asahi Shimbun newspape...\n",
      "Cluster 7: \"This type of very clear sort of crime has been co...\n",
      "Cluster 8: A poll by the Mainichi Shimbun newspaper found 79%...\n",
      "Cluster 9: \"The easiest way to understand factions is that th...\n",
      "Cluster 10: But he says Japan must overhaul the selection of p...\n",
      "Cluster 11: Critics accused Abe of trying to extend the retire...\n",
      "Cluster 12: But Izumi believes the result of the scandal will ...\n",
      "Cluster 13: They can't arrest lawmakers while parliament is in...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "clusters = process_text(article_text)\n",
    "\n",
    "for i, cluster in enumerate(clusters, 1):\n",
    "    print(f\"Cluster {i}: {cluster[:50]}...\")  # Prints the first 50 chars of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Party bosses fall in Japan\\'s worst political corruption scandal in decades\\nParty bosses fall in Japan\\'s worst political corruption scandal in decades\\nEnlarge this image toggle caption Kyodo via Reuters Connect Kyodo via Reuters Connect\\nSEOUL, South Korea â€” Japan\\'s governing Liberal Democratic Party (LDP) on Friday replaced two of its top executives, as part of a purge related to the worst corruption scandal to rock the country in three decades.\\nThe outgoing executives were in charge of policy and parliamentary affairs. They belonged to an LDP faction â€” a sort of party within a party â€” formerly led by the late ex-Prime Minister Shinzo Abe.\\nJapan\\'s current prime minister, Fumio Kishida, has been sacking party chiefs and Cabinet members to save his administration, as prosecutors investigate allegations that LDP ministers and lawmakers violated political finance laws.\\n\"Japanese democracy\\'s strength is going to be tested,\" says Hitoshi Tanaka, a former diplomat and special adviser to the Japan Research Institute, a consultancy and think tank.\\nPrevious corruption scandals involving factions and money have led to the downfall of earlier administrations and Tanaka says he has \"the feeling that the current political funds scandal may be deep enough to lead to regime change in this country.\"\\nThe LDP has only lost power twice in seven decades, the opposition remains weak and divided, and it would take a massively damaging scandal to allow the opposition to take power.\\nA change of administration could lead to policy change, including toward the United States. But Tanaka says support for Japan\\'s alliance with the U.S. is likely to remain solid.\\nProsecutors, meanwhile, are looking into allegations that ministers and lawmakers took kickbacks for political funds they raised and poured millions of dollars in fundraising proceeds into slush funds. Kickbacks are not illegal in Japan, but must be recorded and reported, which the LDP politicians allegedly did not do.\\nSince the allegations came to light in November, several LDP politicians have resigned. In a statement cited by The Asahi Shimbun newspaper, the faction formerly led by Abe said, \"We sincerely apologize for eroding trust in politics. We will provide utmost cooperation to the investigation and respond to it with sincerity.\" Other factions expressed similar sentiments, the newspaper said.\\nTanaka says Abe was so powerful that his faction members apparently thought they could get away with flouting political finance laws.\\n\"This type of very clear sort of crime has been conducted for such a long period of time,\" he says, \"and I guess that this is very much to do with the abuse of power.\"\\nPrime Minister Kishida told reporters last week he felt a \"strong sense of crisis\" because of the scandal, and pledged to \"work like a ball of fire\" to regain the public\\'s trust.\\nBut opinion polls show overwhelming disapproval of his handling of the debacle. A poll by the Mainichi Shimbun newspaper found 79% of respondents disapprove of his performance â€” worse than any Japanese leader in more than seven decades.\\nSince its creation 68 years ago, the LDP has been made up of competing factions. Faction bosses reward members with government posts and support at election time. Members support bosses by raising funds, voting for bills and voting for the faction heads to become the party\\'s president, who usually becomes prime minister of Japan.\\n\"The easiest way to understand factions is that they are basically groups who try to make their leader prime minister,\" says veteran political journalist Hiroshi Izumi. He says that behind the current scandal is a struggle among the factions.\\nTanaka argues that the LDP\\'s factional infighting of the past has been replaced by \"a much more sort of amicable process\" for selecting prime ministers. But he says Japan must overhaul the selection of party leaders, and do away with horse-trading among faction bosses behind closed doors.\\n\"There is a need for much more open, competitive process to produce the political leader in this country,\" he says.\\nIzumi, though, says that the struggle is not just among factions, but between the prime minister and the judiciary.\\nCritics accused Abe of trying to extend the retirement age of top prosecutors, in order to have his ally in a position to shield him from corruption scandals.\\nIzumi says that before Abe was assassinated in July 2022, the leader was too powerful for the prosecutors to touch.\\n\"Now it\\'s been about a year and a half since Abe was shot and died,\" he says. \"Just as prosecutors were planning to take their revenge, the slush fund case fell in their lap.\"\\nThere\\'s no public evidence of the feud Izumi describes. But Izumi believes the result of the scandal will be the end of the Abe faction, and a big shift in power within the LDP.\\nAnd that, he adds, may overshadow anything else Prime Minister Kishida achieves.\\n\"He would put an end to Abe\\'s dictatorial politics,\" Izumi says. \"And Abe\\'s faction, which symbolizes those politics, would be destroyed. I think that would be Kishida\\'s legacy as prime minister.\"\\nThe outcome, though, depends somewhat on the prosecutors. And they are up against the clock.\\nThey can\\'t arrest lawmakers while parliament is in session, so that gives them until the legislature reopens sometime in January to build their cases and go after their targets.\\nChie Kobayashi contributed to this report in Tokyo.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.0023160173160173144, subjectivity=0.39767316017316023)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(article_text)\n",
    "\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.3441440463066101,\n",
       "  0.21763548254966736,\n",
       "  0.20591646432876587,\n",
       "  0.13564704358577728],\n",
       " [0.8734523057937622,\n",
       "  0.48352286219596863,\n",
       "  0.15992116928100586,\n",
       "  0.012142530642449856])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_labels = [\"Politics\", \"Economy\", \"Environment\", 'Others']\n",
    "classes_class(article_text, candidate_labels, multi_label=True)['scores'], classes_class(article.description, candidate_labels, multi_label=True)['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of elements in the cluster with higher data: [0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "candidate_labels = [\"Politics\", \"Economy\", \"Environment\", 'Others']\n",
    "\n",
    "output = classes_class(\n",
    "    '''\n",
    "    Machine Learning is a great job\n",
    "    '''\n",
    "    , candidate_labels, multi_label=True)\n",
    "\n",
    "data_train = np.array(output['scores']).reshape(-1, 1)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "\n",
    "# Fit the model\n",
    "kmeans.fit(data_train)\n",
    "\n",
    "# Cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Calculate the average value of each cluster\n",
    "cluster_averages = [data_train[labels == i].mean() for i in range(2)]\n",
    "\n",
    "# Identify the cluster with the higher average value\n",
    "higher_avg_cluster = np.argmax(cluster_averages)\n",
    "\n",
    "# Indices of elements in the higher data cluster\n",
    "indices_higher_cluster = np.where(labels == higher_avg_cluster)[0]\n",
    "\n",
    "print(\"Indices of elements in the cluster with higher data:\", indices_higher_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': '\\n    Machine Learning is a great job\\n    ',\n",
       " 'labels': ['Others', 'Economy', 'Environment', 'Politics'],\n",
       " 'scores': [0.020331770181655884,\n",
       "  0.0007957927300594747,\n",
       "  0.00040434286347590387,\n",
       "  0.00037566403625532985]}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "candidate_labels = [\"Politics\", \"Economy\", \"Environment\", 'Others']\n",
    "\n",
    "output = classes_class(\n",
    "    '''\n",
    "    Machine Learning is going to boom the would economy, especially data science and the new natural language processing models\n",
    "    '''\n",
    "    , candidate_labels, multi_label=True)\n",
    "\n",
    "data_train = np.array(output['scores']).reshape(-1, 1)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "\n",
    "# Fit the model\n",
    "kmeans.fit(data_train)\n",
    "\n",
    "# Cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Calculate the average value of each cluster\n",
    "cluster_averages = [data_train[labels == i].mean() for i in range(2)]\n",
    "higher_avg_cluster = np.argmax(cluster_averages)\n",
    "indices_higher_cluster = np.where(labels == higher_avg_cluster)[0]\n",
    "\n",
    "index_low_cluster_start = indices_higher_cluster[-1] + 1\n",
    "valid = []\n",
    "MAX_SELECTED = 2\n",
    "for i, label in enumerate(output['labels']):\n",
    "    if i < index_low_cluster_start:\n",
    "        if i < MAX_SELECTED:\n",
    "            if label != \"Others\":\n",
    "                valid.append(label)\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Economy'],\n",
       " {'sequence': '\\n    Machine Learning is going to boom the would economy, especially data science and the new natural language processing models\\n    ',\n",
       "  'labels': ['Economy', 'Others', 'Environment', 'Politics'],\n",
       "  'scores': [0.5153793096542358,\n",
       "   0.012050898745656013,\n",
       "   0.00041549428715370595,\n",
       "   0.00041548116132616997]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': '\\n    Machine Learning is a great job\\n    ',\n",
       " 'labels': ['Others', 'Economy', 'Environment', 'Politics'],\n",
       " 'scores': [0.020331770181655884,\n",
       "  0.0007957927300594747,\n",
       "  0.00040434286347590387,\n",
       "  0.00037566403625532985]}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No category\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_11328\\1259493301.py:34: DeprecationWarning: Calling nonzero on 0d arrays is deprecated, as it behaves surprisingly. Use `atleast_1d(cond).nonzero()` if the old behavior was intended. If the context of this warning is of the form `arr[nonzero(cond)]`, just use `arr[cond]`.\n",
      "  others_index = np.where(output['labels'] == 'Others')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "candidate_labels = [\"Politics\", \"Economy\", \"Environment\", 'Others']\n",
    "\n",
    "output = classes_class(\n",
    "    '''\n",
    "    Machine Learning is a great job\n",
    "    '''\n",
    "    , candidate_labels, multi_label=True)\n",
    "\n",
    "data_train = np.array(output['scores']).reshape(-1, 1)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "\n",
    "# Fit the model\n",
    "kmeans.fit(data_train)\n",
    "\n",
    "# Cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Calculate the average value of each cluster\n",
    "cluster_averages = [data_train[labels == i].mean() for i in range(2)]\n",
    "\n",
    "# Identify the cluster with the higher average value\n",
    "higher_avg_cluster = np.argmax(cluster_averages)\n",
    "\n",
    "# Indices of elements in the higher data cluster\n",
    "indices_higher_cluster = np.where(labels == higher_avg_cluster)[0]\n",
    "\n",
    "\n",
    "labels = np.array(output['labels'])\n",
    "scores = np.array(output['scores'])\n",
    "others_index = np.where(output['labels'] == 'Others')\n",
    "if labels[indices_higher_cluster][0] == \"Others\":\n",
    "    print(\"No category\")\n",
    "else:\n",
    "    for i in range(len(output['labels'])):\n",
    "        if \"Others\" in labels[indices_higher_cluster]:\n",
    "            if labels[np.argmax(scores[indices_higher_cluster])] == \"Others\":\n",
    "                print(\"No category, right cluster but argmax is Others\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9211400151252747,\n",
       " 0.6830674409866333,\n",
       " 0.2604702413082123,\n",
       " 0.1896524429321289,\n",
       " 0.16527467966079712,\n",
       " 0.1205194815993309,\n",
       " 0.09266605228185654,\n",
       " 0.07585158944129944,\n",
       " 0.017313361167907715,\n",
       " 0.006902643013745546]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_class(article.description, candidate_labels_2[:10], multi_label=True)['scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/classes_classifier\\\\tokenizer_config.json',\n",
       " './models/classes_classifier\\\\special_tokens_map.json',\n",
       " './models/classes_classifier\\\\spm.model',\n",
       " './models/classes_classifier\\\\added_tokens.json',\n",
       " './models/classes_classifier\\\\tokenizer.json')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_class.model.save_pretrained('./models/emotions_classifier')\n",
    "emotions_class.tokenizer.save_pretrained('./models/emotions_classifier')\n",
    "\n",
    "classes_class.model.save_pretrained('./models/classes_classifier')\n",
    "classes_class.tokenizer.save_pretrained('./models/classes_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the emotion classification model\n",
    "emotion_model_path = './models/emotions_classifier'\n",
    "\n",
    "# Load the zero-shot classification model\n",
    "classification_model_path = './models/classes_classifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "emotion_tokenizer = AutoTokenizer.from_pretrained(emotion_model_path)\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(emotion_model_path)\n",
    "emotions_class = pipeline(\"text-classification\", model=emotion_model, tokenizer=emotion_tokenizer, return_all_scores=True)\n",
    "\n",
    "classification_tokenizer = AutoTokenizer.from_pretrained(classification_model_path)\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(classification_model_path)\n",
    "classes_class = pipeline(\"zero-shot-classification\", model=classification_model, tokenizer=classification_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2023, 12, 24, 17, 1, 20),\n",
       " datetime.datetime(2023, 12, 24, 6, 24, 9))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.date_download, article.date_publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__subclasshook__',\n",
       " '__weakref__',\n",
       " 'authors',\n",
       " 'date_download',\n",
       " 'date_modify',\n",
       " 'date_publish',\n",
       " 'description',\n",
       " 'filename',\n",
       " 'get_dict',\n",
       " 'get_serializable_dict',\n",
       " 'image_url',\n",
       " 'language',\n",
       " 'localpath',\n",
       " 'maintext',\n",
       " 'source_domain',\n",
       " 'text',\n",
       " 'title',\n",
       " 'title_page',\n",
       " 'title_rss',\n",
       " 'url']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(article)[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NewsScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = NewsPlease.from_url(\"https://www.npr.org/2023/12/22/1221230635/japan-alleged-political-corruption-ldp-slush-fund\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bbc.com/news/world-middle-east-67814475\n",
      "https://www.wsj.com/world/middle-east/israeli-soldier-death-toll-grows-as-hamas-shifts-to-guerilla-attacks-dee2bc9b\n",
      "https://www.timesofisrael.com/idf-says-8-more-soldiers-killed-amid-heavy-fighting-in-south-central-gaza-saturday/\n",
      "https://www.nytimes.com/live/2023/12/24/world/israel-hamas-war-gaza-news\n",
      "https://www.clickorlando.com/news/local/2023/12/24/victim-idd-person-of-interest-revealed-after-deadly-shooting-at-mall-in-ocala/\n",
      "https://www.fox35orlando.com/news/active-shooter-reported-at-ocala-mall-police-say\n",
      "https://www.usatoday.com/story/news/nation/2023/12/24/shooting-erupts-ocala-florida-mall/72024171007/\n",
      "https://www.orlandosentinel.com/2023/12/23/a-man-is-killed-and-a-woman-injured-in-a-targeted-afternoon-shooting-at-a-florida-shopping-mall/\n",
      "https://thehill.com/policy/defense/4375673-iranian-drone-struck-chemical-tanker-in-indian-ocean-pentagon/\n",
      "https://apnews.com/article/bethlehem-christmas-israel-hamas-war-e408a3e48d18c69a7a0b018505cdc732\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "# URL of the Google News page you want to scrape\n",
    "url = 'https://news.google.com/topstories?hl=en-US&gl=US&ceid=US:en'\n",
    "\n",
    "# Headers to simulate a real user visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "for link in soup.find_all('article'):\n",
    "    a_tag = link.find('a', href=True)\n",
    "    if a_tag and 'href' in a_tag.attrs:\n",
    "        # Get the partial URL\n",
    "        partial_url = a_tag.attrs['href']\n",
    "        # Construct the full Google News URL\n",
    "        google_news_url = f'https://news.google.com{partial_url}'\n",
    "\n",
    "        response = requests.get(google_news_url, allow_redirects=True, timeout=10)\n",
    "        \n",
    "        # Get the final URL after redirection\n",
    "        final_url = response.url\n",
    "        \n",
    "        print(final_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = NewsPlease.from_url(\"https://www.bbc.com/news/world-middle-east-67814475\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Israeli PM says his troops will keep fighting - after one of their deadliest days of the conflict.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0460f6db6993a86a65738508dba0aca00af9523ba4143ef22f5162384401bb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
