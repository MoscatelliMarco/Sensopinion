{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import logging\n",
    "date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n",
    "                    datefmt=date_format)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from newsplease import NewsPlease\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from variables import categories, subcategories, black_listed_domains\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import copy\n",
    "import nltk\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-02-28 07:49:29 - __main__ - INFO - Load emotion classifier\n",
      "2024-02-28 07:49:30 - __main__ - INFO - Load category classifier\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Set transformers' logging level to WARNING to suppress debug logs\n",
    "logging.getLogger('newsplease').setLevel(logging.WARNING)\n",
    "\n",
    "nltk.download('punkt')  # Required for the first time\n",
    "\n",
    "# Load the classifiers\n",
    "emotion_model_path = '../models/emotions_classifier'\n",
    "categories_model_path = '../models/classes_classifier'\n",
    "logger.info(\"Load emotion classifier\")\n",
    "emotion_tokenizer = AutoTokenizer.from_pretrained(emotion_model_path)\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(emotion_model_path)\n",
    "emotions_classifier = pipeline(\"text-classification\", model=emotion_model, tokenizer=emotion_tokenizer, top_k=None)\n",
    "logger.info(\"Load category classifier\")\n",
    "categories_tokenizer = AutoTokenizer.from_pretrained(categories_model_path)\n",
    "categories_model = AutoModelForSequenceClassification.from_pretrained(categories_model_path)\n",
    "categories_classifier = pipeline(\"zero-shot-classification\", model=categories_model, tokenizer=categories_tokenizer)\n",
    "\n",
    "def process_article(url, entries):\n",
    "    article = NewsPlease.from_url(url)\n",
    "\n",
    "    for black_listed_domain in black_listed_domains:\n",
    "        if black_listed_domain in url:\n",
    "            return\n",
    "\n",
    "    logger.info(url)\n",
    "    \n",
    "    # If article is a dict that means that the request was negative\n",
    "    if isinstance(article, dict):\n",
    "        raise Exception(f\"Error scraping a website: {url}\")\n",
    "\n",
    "    article_text = article.maintext\n",
    "    article_title = article.title\n",
    "    article_description = article.description\n",
    "    article_date_publish = article.date_publish\n",
    "    article_image = article.image_url\n",
    "\n",
    "    if not article_description:\n",
    "        logger.info(f\"Article invalid no description: {article.url}\")\n",
    "        return\n",
    "    if not article_text:\n",
    "        logger.info(f\"Article invalid no maintext: {article.url}\")\n",
    "        return\n",
    "    if not article_date_publish:\n",
    "        logger.info(f\"Article invalid no date_publish: {article.url}\")\n",
    "        return\n",
    "    if article_date_publish < datetime.now() - timedelta(days=int(os.environ.get(\"ACCEPTED_DAYS_NEWS\"))):\n",
    "        logger.info(f\"Article invalid date publish older than {int(os.environ.get('ACCEPTED_DAYS_NEWS'))} days: {article.url}\")\n",
    "        return\n",
    "    if not article_title:\n",
    "        logger.info(f\"Article invalid no title: {article.url}\")\n",
    "        return\n",
    "    if not article_image:\n",
    "        logger.info(f\"Article invalid no image_url: {article.url}\")\n",
    "        return\n",
    "    if 'news.google.com' in url:\n",
    "        logger.info(f\"Article invalid url news.google.com domain: {article.url}\")\n",
    "        return\n",
    "\n",
    "    for entry in entries:\n",
    "        if article_title == entry['title']:\n",
    "            logger.info(f\"Article invalid, same title already present inside the database: {article.url}\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        clusters = process_text(article_text)\n",
    "    except:\n",
    "        logger.info(\"A sentence or more in the article exceed the max amount of 512 chars\")\n",
    "        return\n",
    "\n",
    "    # Categories\n",
    "    logger.debug(\"Analyzing categories\")\n",
    "    valid_categories, valid_subcategories = pick_categories(article_title + \"\\n\" + article_description)\n",
    "\n",
    "    if not len(valid_categories) or not len(valid_subcategories):\n",
    "        logger.info(f\"Category == Others {url}\")\n",
    "        return\n",
    "\n",
    "    logger.debug(\"Clustering\")\n",
    "    clusters_dict = []\n",
    "    for i, cluster in enumerate(clusters, 1):\n",
    "        words = cluster.split()\n",
    "        dict_append = {\n",
    "            'chars': len(cluster), \n",
    "            'words': len(words), \n",
    "            'content': cluster,\n",
    "            'emotions': {},\n",
    "            'sentiment': {}\n",
    "        }\n",
    "        for item in emotions_classifier(cluster)[0]:\n",
    "            dict_append['emotions'][item['label']] = item['score']\n",
    "        blob = TextBlob(article_text)\n",
    "        sentiment = blob.sentiment\n",
    "        dict_append['sentiment']['polarity'] = sentiment.polarity\n",
    "        dict_append['sentiment']['subjectivity'] = sentiment.subjectivity\n",
    "\n",
    "        clusters_dict.append(dict_append)\n",
    "\n",
    "    logger.debug(\"Analyzing emotions\")\n",
    "    # Take weighted everage of emotions\n",
    "    emotion_sum_char = 0\n",
    "    emotion_value = 0\n",
    "    emotions_percentage = {}\n",
    "    for emotion in ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']:\n",
    "        for cluster in clusters_dict:\n",
    "            emotion_value += cluster['chars'] * cluster['emotions'][emotion]\n",
    "            emotion_sum_char += cluster['chars']\n",
    "        emotions_percentage[emotion] = emotion_value / emotion_sum_char\n",
    "        emotion_value = 0\n",
    "        emotion_sum_char = 0\n",
    "    \n",
    "    # Change name of joy to happiness\n",
    "    emotions_percentage['happiness'] = emotions_percentage['joy']\n",
    "    del emotions_percentage['joy']\n",
    "\n",
    "    # Sentiment\n",
    "    sentiment = {}\n",
    "    item_value = 0\n",
    "    item_sum_char = 0\n",
    "    for item in ['polarity', 'subjectivity']:\n",
    "        for cluster in clusters_dict:\n",
    "            item_value += cluster['chars'] * cluster['sentiment'][item]\n",
    "            item_sum_char += cluster['chars']\n",
    "            sentiment[item] = item_value / item_sum_char\n",
    "            item_value = 0\n",
    "            item_sum_char = 0\n",
    "    sentiment['polarity'] = (sentiment['polarity'] + 1) / 2\n",
    "\n",
    "    return emotions_percentage, sentiment, valid_categories, valid_subcategories, article_date_publish, article_image, article_description, article_title\n",
    "\n",
    "    \n",
    "def pick_categories(text, max_selectable_subcategories=5):\n",
    "    categories_an = copy.deepcopy(categories)\n",
    "    categories_an.append(\"Others\")\n",
    "\n",
    "    output = categories_classifier(text, categories_an, multi_label=True)\n",
    "\n",
    "    # Apply kmeans\n",
    "    data_train = np.array(output['scores']).reshape(-1, 1)\n",
    "    kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "    kmeans.fit(data_train)\n",
    "    labels = kmeans.labels_\n",
    "    cluster_averages = [data_train[labels == i].mean() for i in range(2)]\n",
    "    higher_avg_cluster = np.argmax(cluster_averages)\n",
    "    indices_higher_cluster = np.where(labels == higher_avg_cluster)[0]\n",
    "\n",
    "    # Find valid labels\n",
    "    index_low_cluster_start = indices_higher_cluster[-1] + 1\n",
    "    valid_categories = []\n",
    "    for i, label in enumerate(output['labels']):\n",
    "        if i < index_low_cluster_start:\n",
    "            if label != \"Others\":\n",
    "                valid_categories.append(label)\n",
    "            else:\n",
    "                logger.debug(\"Label == Others\")\n",
    "                break\n",
    "\n",
    "    valid_subcategories = []\n",
    "\n",
    "    for category in valid_categories:\n",
    "        category_index = np.where(np.array(categories) == category)[0][0]\n",
    "        output = categories_classifier(text, subcategories[category_index], multi_label=True)\n",
    "\n",
    "        # Apply kmeans\n",
    "        data_train = np.array(output['scores']).reshape(-1, 1)\n",
    "        kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "        kmeans.fit(data_train)\n",
    "        labels = kmeans.labels_\n",
    "        cluster_averages = [data_train[labels == i].mean() for i in range(2)]\n",
    "        higher_avg_cluster = np.argmax(cluster_averages)\n",
    "        indices_higher_cluster = np.where(labels == higher_avg_cluster)[0]\n",
    "\n",
    "        # Find valid labels\n",
    "        index_low_cluster_start = indices_higher_cluster[-1] + 1\n",
    "        valid_subcategories_append = []\n",
    "        for i, label in enumerate(output['labels']):\n",
    "            if i < index_low_cluster_start:\n",
    "                if i < max_selectable_subcategories:\n",
    "                    if label != \"Others\":\n",
    "                        valid_subcategories_append.append(label)\n",
    "                    else:\n",
    "                        if i == 0:\n",
    "                            valid_subcategories_append.append('Others')\n",
    "                        break\n",
    "        valid_subcategories.append(valid_subcategories_append)\n",
    "\n",
    "    return valid_categories, valid_subcategories\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def balance_clusters(clusters):\n",
    "    # Define a threshold for when to redistribute (e.g., if the last cluster is less than half the average size)\n",
    "    average_size = sum(len(cluster) for cluster in clusters) / len(clusters)\n",
    "    min_size = average_size / 2\n",
    "\n",
    "    if len(clusters) > 1 and len(clusters[-1]) < min_size:\n",
    "        # Attempt to redistribute\n",
    "        last_cluster = clusters[-1].split()\n",
    "        prev_cluster = clusters[-2].split()\n",
    "\n",
    "        # While the last cluster is too short and the previous cluster has sentences to give\n",
    "        while len(' '.join(last_cluster)) < min_size and prev_cluster:\n",
    "            # Move the last sentence from the previous cluster to the beginning of the last cluster\n",
    "            last_cluster.insert(0, prev_cluster.pop())\n",
    "\n",
    "        # Update the clusters with the redistributed sentences\n",
    "        clusters[-2] = ' '.join(prev_cluster)\n",
    "        clusters[-1] = ' '.join(last_cluster)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def create_clusters(sentences):\n",
    "    clusters = []\n",
    "    current_cluster = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Check if adding the next sentence would exceed the limit\n",
    "        if len(current_cluster) + len(sentence) > 512:\n",
    "            # If the current cluster is not empty, add it to clusters\n",
    "            if current_cluster:\n",
    "                clusters.append(current_cluster)\n",
    "            # Start a new cluster with the current sentence\n",
    "            current_cluster = sentence\n",
    "        else:\n",
    "            # Add a space if the cluster already has content\n",
    "            if current_cluster:\n",
    "                current_cluster += \" \"\n",
    "            current_cluster += sentence\n",
    "\n",
    "    # Don't forget to add the last cluster if it's not empty\n",
    "    if current_cluster:\n",
    "        clusters.append(current_cluster)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def process_text(article):\n",
    "    sentences = split_into_sentences(article)\n",
    "    initial_clusters = create_clusters(sentences)\n",
    "    balanced_clusters = balance_clusters(initial_clusters)\n",
    "    return balanced_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import logging\n",
    "date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n",
    "                    datefmt=date_format)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from newsplease import NewsPlease\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from variables import categories, subcategories, black_listed_domains\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import copy\n",
    "import nltk\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW LIBRARIES\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW FUNCTIONS\n",
    "def get_html_content(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        # Check if request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Return the HTML content of the page\n",
    "            return response.text\n",
    "        else:\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return None\n",
    "\n",
    "def search_div_content(html_content, classes, element_type):\n",
    "    try:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find the specific div element with the provided classes\n",
    "        div_element = soup.find(element_type, class_=classes)\n",
    "        if div_element:\n",
    "            # Return the content of the div element\n",
    "            return div_element.text.strip()\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def get_element_content(url, div_classes, element_type = 'div'):\n",
    "    html_content = get_html_content(url)\n",
    "    return search_div_content(html_content, div_classes, element_type)\n",
    "\n",
    "def string_to_datetime(date_string):\n",
    "    # List of datetime formats to try for parsing the date string\n",
    "    formats_to_try = [\n",
    "        '%B %d, %Y, %I:%M %p',    # Month Day, Year, Hour:Minute AM/PM\n",
    "        '%B %d, %Y %H:%M',        # Month Day, Year, Hour:Minute (24-hour)\n",
    "        '%b %d, %Y - %H:%M',      # Month, Day, Year - Hour:Minute\n",
    "        '%b %d %Y',               # Month, Day, Year\n",
    "        '%Y-%m-%d %I:%M %p',      # Year-Month-Day, Hour:Minute AM/PM\n",
    "        '%Y-%m-%d %H:%M',         # Year-Month-Day, Hour:Minute (24-hour)\n",
    "        '%Y-%m-%d',               # Year-Month-Day\n",
    "    ]\n",
    "    \n",
    "    # Iterate through the formats and try parsing the date string\n",
    "    for format_str in formats_to_try:\n",
    "        try:\n",
    "            # Try to parse the date string using the current format\n",
    "            date_time = datetime.strptime(date_string, format_str)\n",
    "            return date_time\n",
    "        except ValueError:\n",
    "            # If parsing fails, try the next format\n",
    "            continue\n",
    "    \n",
    "    # If none of the formats work, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-02-28 07:49:33 - __main__ - INFO - Load emotion classifier\n",
      "2024-02-28 07:49:33 - __main__ - INFO - Load category classifier\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Set transformers' logging level to WARNING to suppress debug logs\n",
    "logging.getLogger('newsplease').setLevel(logging.WARNING)\n",
    "\n",
    "nltk.download('punkt')  # Required for the first time\n",
    "\n",
    "# Load the classifiers\n",
    "emotion_model_path = '../models/emotions_classifier'\n",
    "categories_model_path = '../models/classes_classifier'\n",
    "logger.info(\"Load emotion classifier\")\n",
    "emotion_tokenizer = AutoTokenizer.from_pretrained(emotion_model_path)\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(emotion_model_path)\n",
    "emotions_classifier = pipeline(\"text-classification\", model=emotion_model, tokenizer=emotion_tokenizer, top_k=None)\n",
    "logger.info(\"Load category classifier\")\n",
    "categories_tokenizer = AutoTokenizer.from_pretrained(categories_model_path)\n",
    "categories_model = AutoModelForSequenceClassification.from_pretrained(categories_model_path)\n",
    "categories_classifier = pipeline(\"zero-shot-classification\", model=categories_model, tokenizer=categories_tokenizer)\n",
    "\n",
    "def process_article(url, entries):\n",
    "    article = NewsPlease.from_url(url)\n",
    "\n",
    "    for black_listed_domain in black_listed_domains:\n",
    "        if black_listed_domain in url:\n",
    "            return\n",
    "\n",
    "    logger.info(url)\n",
    "    \n",
    "    # If article is a dict that means that the request was negative\n",
    "    if isinstance(article, dict):\n",
    "        raise Exception(f\"Error scraping a website: {url}\")\n",
    "\n",
    "    article_text = article.maintext\n",
    "    article_title = article.title\n",
    "    article_description = article.description\n",
    "    article_date_publish = article.date_publish\n",
    "    article_image = article.image_url\n",
    "\n",
    "    # Special cases handling\n",
    "    is_url_special_case = False\n",
    "\n",
    "    for url_case in ['abcnews.go.com', 'english.kyodonews.net', 'businessinsider.in']:\n",
    "        if url_case in url:\n",
    "            is_url_special_case = True\n",
    "\n",
    "    if (not is_url_special_case):\n",
    "        if not article_description:\n",
    "            logger.info(f\"Article invalid no description: {article.url}\")\n",
    "            return\n",
    "        if not article_text:\n",
    "            logger.info(f\"Article invalid no maintext: {article.url}\")\n",
    "            return\n",
    "        if not article_date_publish:\n",
    "            logger.info(f\"Article invalid no date_publish: {article.url}\")\n",
    "            return\n",
    "        if article_date_publish < datetime.now() - timedelta(days=int(os.environ.get(\"ACCEPTED_DAYS_NEWS\"))):\n",
    "            logger.info(f\"Article invalid date publish older than {int(os.environ.get('ACCEPTED_DAYS_NEWS'))} days: {article.url}\")\n",
    "            return\n",
    "        if not article_title:\n",
    "            logger.info(f\"Article invalid no title: {article.url}\")\n",
    "            return\n",
    "        if not article_image:\n",
    "            logger.info(f\"Article invalid no image_url: {article.url}\")\n",
    "            return\n",
    "        if 'news.google.com' in url:\n",
    "            logger.info(f\"Article invalid url news.google.com domain: {article.url}\")\n",
    "            return \n",
    "    else:\n",
    "        content = None;\n",
    "        if ('abcnews.go.com' in url):\n",
    "            content = get_element_content(url, \"xAPpq ZdbeE  jTKbV pCRh\".split()) # Classes that represent the real div containing the date the article was published\n",
    "        if ('english.kyodonews.net' in url):\n",
    "            content = get_element_content(url, \"credit\", element_type='p')\n",
    "            # Define the regex pattern to match the date format\n",
    "            date_pattern = r'(?P<month>\\w{3})\\s+(?P<day>\\d{1,2}),\\s+(?P<year>\\d{4})\\s+-\\s+(?P<hour>\\d{1,2}):(?P<minute>\\d{2})\\b'\n",
    "            \n",
    "            # Search for the date pattern in the string\n",
    "            try:\n",
    "                match = re.search(date_pattern, content)\n",
    "                content = match.group(0)\n",
    "            except:\n",
    "                logger.info(f\"Cannot estrapolate data {url}\")\n",
    "                return\n",
    "        if (\"businessinsider.in\" in url):\n",
    "            content = get_element_content(url, \"Date\", element_type='span')\n",
    "            content = content.split(',')[0] + content.split(',')[1]\n",
    "\n",
    "        print(content)\n",
    "        if not content:\n",
    "            logger.info(f\"Cannot find valid date_published {url}\")\n",
    "            return\n",
    "        cache_datetime = string_to_datetime(content)\n",
    "\n",
    "        if cache_datetime:\n",
    "            article_date_publish = string_to_datetime(content)\n",
    "        else:\n",
    "            logger.info(f\"Invalid date format {url}\")\n",
    "            return\n",
    "        \n",
    "        print(article_date_publish)\n",
    "\n",
    "    # Check later if the date is valid \n",
    "    if article_date_publish > datetime.now():\n",
    "        logger.info(f\"Article date is in the future: {article.url}\")\n",
    "        return   \n",
    "\n",
    "    for entry in entries:\n",
    "        if article_title == entry['title']:\n",
    "            logger.info(f\"Article invalid, same title already present inside the database: {article.url}\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        clusters = process_text(article_text)\n",
    "    except:\n",
    "        logger.info(\"A sentence or more in the article exceed the max amount of 512 chars\")\n",
    "        return\n",
    "\n",
    "    # Categories\n",
    "    logger.debug(\"Analyzing categories\")\n",
    "    valid_categories, valid_subcategories = pick_categories(article_title + \"\\n\" + article_description)\n",
    "\n",
    "    if not len(valid_categories) or not len(valid_subcategories):\n",
    "        logger.info(f\"Category == Others {url}\")\n",
    "        return\n",
    "\n",
    "    logger.debug(\"Clustering\")\n",
    "    clusters_dict = []\n",
    "    for i, cluster in enumerate(clusters, 1):\n",
    "        words = cluster.split()\n",
    "        dict_append = {\n",
    "            'chars': len(cluster), \n",
    "            'words': len(words), \n",
    "            'content': cluster,\n",
    "            'emotions': {},\n",
    "            'sentiment': {}\n",
    "        }\n",
    "        for item in emotions_classifier(cluster)[0]:\n",
    "            dict_append['emotions'][item['label']] = item['score']\n",
    "        blob = TextBlob(article_text)\n",
    "        sentiment = blob.sentiment\n",
    "        dict_append['sentiment']['polarity'] = sentiment.polarity\n",
    "        dict_append['sentiment']['subjectivity'] = sentiment.subjectivity\n",
    "\n",
    "        clusters_dict.append(dict_append)\n",
    "\n",
    "    logger.debug(\"Analyzing emotions\")\n",
    "    # Take weighted everage of emotions\n",
    "    emotion_sum_char = 0\n",
    "    emotion_value = 0\n",
    "    emotions_percentage = {}\n",
    "    for emotion in ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']:\n",
    "        for cluster in clusters_dict:\n",
    "            emotion_value += cluster['chars'] * cluster['emotions'][emotion]\n",
    "            emotion_sum_char += cluster['chars']\n",
    "        emotions_percentage[emotion] = emotion_value / emotion_sum_char\n",
    "        emotion_value = 0\n",
    "        emotion_sum_char = 0\n",
    "    \n",
    "    # Change name of joy to happiness\n",
    "    emotions_percentage['happiness'] = emotions_percentage['joy']\n",
    "    del emotions_percentage['joy']\n",
    "\n",
    "    # Sentiment\n",
    "    sentiment = {}\n",
    "    item_value = 0\n",
    "    item_sum_char = 0\n",
    "    for item in ['polarity', 'subjectivity']:\n",
    "        for cluster in clusters_dict:\n",
    "            item_value += cluster['chars'] * cluster['sentiment'][item]\n",
    "            item_sum_char += cluster['chars']\n",
    "            sentiment[item] = item_value / item_sum_char\n",
    "            item_value = 0\n",
    "            item_sum_char = 0\n",
    "    sentiment['polarity'] = (sentiment['polarity'] + 1) / 2\n",
    "\n",
    "    return emotions_percentage, sentiment, valid_categories, valid_subcategories, article_date_publish, article_image, article_description, article_title\n",
    "\n",
    "    \n",
    "def pick_categories(text, max_selectable_subcategories=5):\n",
    "    categories_an = copy.deepcopy(categories)\n",
    "    categories_an.append(\"Others\")\n",
    "\n",
    "    output = categories_classifier(text, categories_an, multi_label=True)\n",
    "\n",
    "    # Apply kmeans\n",
    "    data_train = np.array(output['scores']).reshape(-1, 1)\n",
    "    kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "    kmeans.fit(data_train)\n",
    "    labels = kmeans.labels_\n",
    "    cluster_averages = [data_train[labels == i].mean() for i in range(2)]\n",
    "    higher_avg_cluster = np.argmax(cluster_averages)\n",
    "    indices_higher_cluster = np.where(labels == higher_avg_cluster)[0]\n",
    "\n",
    "    # Find valid labels\n",
    "    index_low_cluster_start = indices_higher_cluster[-1] + 1\n",
    "    valid_categories = []\n",
    "    for i, label in enumerate(output['labels']):\n",
    "        if i < index_low_cluster_start:\n",
    "            if label != \"Others\":\n",
    "                valid_categories.append(label)\n",
    "            else:\n",
    "                logger.debug(\"Label == Others\")\n",
    "                break\n",
    "\n",
    "    valid_subcategories = []\n",
    "\n",
    "    for category in valid_categories:\n",
    "        category_index = np.where(np.array(categories) == category)[0][0]\n",
    "        output = categories_classifier(text, subcategories[category_index], multi_label=True)\n",
    "\n",
    "        # Apply kmeans\n",
    "        data_train = np.array(output['scores']).reshape(-1, 1)\n",
    "        kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "        kmeans.fit(data_train)\n",
    "        labels = kmeans.labels_\n",
    "        cluster_averages = [data_train[labels == i].mean() for i in range(2)]\n",
    "        higher_avg_cluster = np.argmax(cluster_averages)\n",
    "        indices_higher_cluster = np.where(labels == higher_avg_cluster)[0]\n",
    "\n",
    "        # Find valid labels\n",
    "        index_low_cluster_start = indices_higher_cluster[-1] + 1\n",
    "        valid_subcategories_append = []\n",
    "        for i, label in enumerate(output['labels']):\n",
    "            if i < index_low_cluster_start:\n",
    "                if i < max_selectable_subcategories:\n",
    "                    if label != \"Others\":\n",
    "                        valid_subcategories_append.append(label)\n",
    "                    else:\n",
    "                        if i == 0:\n",
    "                            valid_subcategories_append.append('Others')\n",
    "                        break\n",
    "        valid_subcategories.append(valid_subcategories_append)\n",
    "\n",
    "    return valid_categories, valid_subcategories\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def balance_clusters(clusters):\n",
    "    # Define a threshold for when to redistribute (e.g., if the last cluster is less than half the average size)\n",
    "    average_size = sum(len(cluster) for cluster in clusters) / len(clusters)\n",
    "    min_size = average_size / 2\n",
    "\n",
    "    if len(clusters) > 1 and len(clusters[-1]) < min_size:\n",
    "        # Attempt to redistribute\n",
    "        last_cluster = clusters[-1].split()\n",
    "        prev_cluster = clusters[-2].split()\n",
    "\n",
    "        # While the last cluster is too short and the previous cluster has sentences to give\n",
    "        while len(' '.join(last_cluster)) < min_size and prev_cluster:\n",
    "            # Move the last sentence from the previous cluster to the beginning of the last cluster\n",
    "            last_cluster.insert(0, prev_cluster.pop())\n",
    "\n",
    "        # Update the clusters with the redistributed sentences\n",
    "        clusters[-2] = ' '.join(prev_cluster)\n",
    "        clusters[-1] = ' '.join(last_cluster)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def create_clusters(sentences):\n",
    "    clusters = []\n",
    "    current_cluster = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Check if adding the next sentence would exceed the limit\n",
    "        if len(current_cluster) + len(sentence) > 512:\n",
    "            # If the current cluster is not empty, add it to clusters\n",
    "            if current_cluster:\n",
    "                clusters.append(current_cluster)\n",
    "            # Start a new cluster with the current sentence\n",
    "            current_cluster = sentence\n",
    "        else:\n",
    "            # Add a space if the cluster already has content\n",
    "            if current_cluster:\n",
    "                current_cluster += \" \"\n",
    "            current_cluster += sentence\n",
    "\n",
    "    # Don't forget to add the last cluster if it's not empty\n",
    "    if current_cluster:\n",
    "        clusters.append(current_cluster)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def process_text(article):\n",
    "    sentences = split_into_sentences(article)\n",
    "    initial_clusters = create_clusters(sentences)\n",
    "    balanced_clusters = balance_clusters(initial_clusters)\n",
    "    return balanced_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST CASES\n",
    "- https://www.businessinsider.in/stock-market/news/indias-stock-market-will-soar-to-10-trillion-by-2030-and-the-countrys-growth-is-impossible-for-investors-to-ignore-jefferies-says/articleshow/107923933.cms\n",
    "- https://english.kyodonews.net/news/2024/02/28921a5887be-kyodo-news-digest-feb-22-2024.html\n",
    "- https://edition.cnn.com/videos/politics/2024/02/24/donald-trump-south-carolina-republican-primary-speech-lindsey-graham-vpx.cnn\n",
    "- https://abcnews.go.com/International/11-lions-rescued-conflict-hit-sudan-arrive-south/story?id=107358308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 07:49:35 - __main__ - INFO - https://abcnews.go.com/International/11-lions-rescued-conflict-hit-sudan-arrive-south/story?id=107358308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "February 21, 2024, 5:07 AM\n",
      "2024-02-21 05:07:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'anger': 0.007551076454760699,\n",
       "  'disgust': 0.02074077466717026,\n",
       "  'fear': 0.13316106113365736,\n",
       "  'neutral': 0.4200462482855211,\n",
       "  'sadness': 0.28166169620659876,\n",
       "  'surprise': 0.014717545960952845,\n",
       "  'happiness': 0.12212165567343855},\n",
       " {'polarity': 0.5296976461038961, 'subjectivity': 0.37447240259740255},\n",
       " ['Politics'],\n",
       " [['International Relations',\n",
       "   'Defence And Security',\n",
       "   'Policy Reforms',\n",
       "   'Civil Rights',\n",
       "   'Politics Scandals']],\n",
       " datetime.datetime(2024, 2, 21, 5, 7),\n",
       " 'https://i.abcnewsfe.com/a/59cb3af0-f8c9-4d60-b2cf-6a3aaf8dfcf7/lion1-ht-ml-240220_1708431468370_hpMain_16x9.jpg?w=1600',\n",
       " 'The lions -- rescued among a total of 48 animals from Sudan’s capital, Khartoun, the heart of the conflict -- have been transferred to the LionsRock Big Cat Sanctuary.',\n",
       " '11 lions rescued from conflict-hit Sudan arrive in South Africa')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_article(\"https://abcnews.go.com/International/11-lions-rescued-conflict-hit-sudan-arrive-south/story?id=107358308\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 07:49:39 - __main__ - INFO - https://english.kyodonews.net/news/2024/02/28921a5887be-kyodo-news-digest-feb-22-2024.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feb 22, 2024 - 23:00\n",
      "2024-02-22 23:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'anger': 0.15132148958822939,\n",
       "  'disgust': 0.11085075157234595,\n",
       "  'fear': 0.1930581207323085,\n",
       "  'neutral': 0.2861747692031231,\n",
       "  'sadness': 0.056245085458391686,\n",
       "  'surprise': 0.05207992317413173,\n",
       "  'happiness': 0.15026986277810425},\n",
       " {'polarity': 0.4971638257575758, 'subjectivity': 0.29418244949494954},\n",
       " ['Politics', 'Economy'],\n",
       " [['Legislation',\n",
       "   'Defence And Security',\n",
       "   'International Relations',\n",
       "   'Elections'],\n",
       "  ['Banking',\n",
       "   'Corporate Finance',\n",
       "   'Real Estate',\n",
       "   'Global Economy',\n",
       "   'Economic Policies']],\n",
       " datetime.datetime(2024, 2, 22, 23, 0),\n",
       " 'https://img.kyodonews.net/english/public/images/posts/fa28d1a87f5377fc469d5719dd202682/photo_l.jpg',\n",
       " 'Here is the latest list of selected news summaries by Kyodo News.',\n",
       " 'Kyodo News Digest: Feb. 22, 2024')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_article(\"https://english.kyodonews.net/news/2024/02/28921a5887be-kyodo-news-digest-feb-22-2024.html\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 07:49:45 - __main__ - INFO - https://www.businessinsider.in/stock-market/news/indias-stock-market-will-soar-to-10-trillion-by-2030-and-the-countrys-growth-is-impossible-for-investors-to-ignore-jefferies-says/articleshow/107923933.cms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feb 23 2024\n",
      "2024-02-23 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'anger': 0.02972278046191557,\n",
       "  'disgust': 0.014421564401263508,\n",
       "  'fear': 0.0239234397788363,\n",
       "  'neutral': 0.6530790249788202,\n",
       "  'sadness': 0.02665264968326409,\n",
       "  'surprise': 0.16645515879799272,\n",
       "  'happiness': 0.08574536521370507},\n",
       " {'polarity': 0.5476871247032538, 'subjectivity': 0.35655983801145097},\n",
       " ['Economy'],\n",
       " [['Stock Market']],\n",
       " datetime.datetime(2024, 2, 23, 0, 0),\n",
       " 'https://www.businessinsider.in/photo/107923933/indias-stock-market-will-soar-to-10-trillion-by-2030-and-the-countrys-growth-is-impossible-for-investors-to-ignore-jefferies-says.jpg?imgsize=519538',\n",
       " \"India's stock market is poised to more than double in value to $10 trillion by 2030, Jefferies says.Jefferies analysts also predicted the Indian economy would\",\n",
       " \"India's stock market will soar to $10 trillion by 2030 and the country's growth is impossible for investors to ignore, Jefferies says\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_article(\"https://www.businessinsider.in/stock-market/news/indias-stock-market-will-soar-to-10-trillion-by-2030-and-the-countrys-growth-is-impossible-for-investors-to-ignore-jefferies-says/articleshow/107923933.cms\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
